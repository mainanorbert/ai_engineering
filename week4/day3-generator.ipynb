{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a9dc3760",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# imports\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import subprocess\n",
        "from IPython.display import Markdown, display\n",
        "import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d6e53673",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4da4fb06",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using open router with any model\n",
        "from openai import OpenAI\n",
        "\n",
        "llm_client = OpenAI(\n",
        "    api_key=openrouter_api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9d8b148c",
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_MODEL = \"gpt-5\"\n",
        "CLAUDE_MODEL = \"anthropic/claude-haiku-4-5\"\n",
        "# GROK_MODEL = \"grok-4\"\n",
        "GEMINI_MODEL = \"google/gemini-2.5-flash-lite\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7cb8b79",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi there! How can I help you today? ðŸ˜Š\n"
          ]
        }
      ],
      "source": [
        "response = llm_client.chat.completions.create(\n",
        "    model= GEMINI_MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hi?\"}],\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489b7a02",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'os': {'system': 'Linux',\n",
              "  'arch': 'x86_64',\n",
              "  'release': '5.10.16.3-microsoft-standard-WSL2',\n",
              "  'version': '#1 SMP Fri Apr 2 22:23:49 UTC 2021',\n",
              "  'kernel': '5.10.16.3-microsoft-standard-WSL2',\n",
              "  'distro': {'name': 'Ubuntu 24.04.1 LTS', 'version': '24.04'},\n",
              "  'wsl': True,\n",
              "  'rosetta2_translated': False,\n",
              "  'target_triple': 'x86_64-linux-gnu'},\n",
              " 'package_managers': ['apt'],\n",
              " 'cpu': {'brand': 'Intel(R) Core(TM) i7-10610U CPU @ 1.80GHz',\n",
              "  'cores_logical': 8,\n",
              "  'cores_physical': 4,\n",
              "  'simd': ['AVX', 'AVX2', 'FMA', 'SSE4_2']},\n",
              " 'toolchain': {'compilers': {'gcc': 'gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0',\n",
              "   'g++': 'g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0',\n",
              "   'clang': '',\n",
              "   'msvc_cl': ''},\n",
              "  'build_tools': {'cmake': '', 'ninja': '', 'make': 'GNU Make 4.3'},\n",
              "  'linkers': {'ld_lld': ''}}}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from system_info import retrieve_system_info\n",
        "\n",
        "# Call retrieve_system_info() to gather detailed information about the current system environment,\n",
        "# including OS, CPU details, toolchains, compilers, and more, and store the result in system_info.\n",
        "# This will help the LLM to choose the best compiler and toolchain for the task\n",
        "system_info = retrieve_system_info()\n",
        "system_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ee5e75b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Youâ€™re already set up. Your system has g++ 13.3.0 installed on Ubuntu 24.04 (WSL2), so you donâ€™t need to install anything to compile and run a single C++ file.\n",
              "\n",
              "Recommended commands for fastest runtime on your machine:\n",
              "- This uses native CPU optimizations, link-time optimization, and disables asserts.\n",
              "\n",
              "Use these in your Python code:\n",
              "- compile_command:\n",
              "  [\"g++\", \"-std=c++20\", \"-Ofast\", \"-march=native\", \"-flto\", \"-DNDEBUG\", \"main.cpp\", \"-o\", \"main\"]\n",
              "- run_command:\n",
              "  [\"./main\"]\n",
              "\n",
              "Notes:\n",
              "- -Ofast may change floating-point behavior for speed. If you need strict IEEE/standard-compliant math, replace -Ofast with -O3.\n",
              "- -march=native produces a binary optimized for this CPU; it may not run on different machines. If you need portability, omit -march=native.\n",
              "\n",
              "Optional (only if you ever need to install g++):\n",
              "- sudo apt update\n",
              "- sudo apt install -y build-essential"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# A system prompt for the code generator \n",
        "\n",
        "message = f\"\"\"\n",
        "Here is a report of the system information for my computer.\n",
        "I want to run a C++ compiler to compile a single C++ file called main.cpp and then execute it in the simplest way possible.\n",
        "Please reply with whether I need to install any C++ compiler to do this. If so, please provide the simplest step by step instructions to do so.\n",
        "\n",
        "If I'm already set up to compile C++ code, then I'd like to run something like this in Python to compile and execute the code:\n",
        "```python\n",
        "compile_command = # something here - to achieve the fastest possible runtime performance\n",
        "compile_result = subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
        "run_command = # something here\n",
        "run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
        "return run_result.stdout\n",
        "```\n",
        "Please tell me exactly what I should use for the compile_command and run_command.\n",
        "\n",
        "System information:\n",
        "{system_info}\n",
        "\"\"\"\n",
        "\n",
        "response = llm_client.chat.completions.create(model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": message}])\n",
        "display(Markdown(response.choices[0].message.content))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2bf7f5ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "compile_command = [\"g++\", \"-std=c++20\", \"-Ofast\", \"-march=native\", \"-flto\", \"-DNDEBUG\", \"main.cpp\", \"-o\", \"main\"]\n",
        "run_command = [\"./main\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b3a78343",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "Your task is to convert Python code into high performance C++ code.\n",
        "Respond only with C++ code. Do not provide any explanation other than occasional comments.\n",
        "The C++ response needs to produce an identical output in the fastest possible time.\n",
        "\"\"\"\n",
        "\n",
        "def user_prompt_for(python):\n",
        "    return f\"\"\"\n",
        "Port this Python code to C++ with the fastest possible implementation that produces identical output in the least time.\n",
        "The system information is:\n",
        "{system_info}\n",
        "Your response will be written to a file called main.cpp and then compiled and executed; the compilation command is:\n",
        "{compile_command}\n",
        "Respond only with C++ code.\n",
        "Python code to port:\n",
        "\n",
        "```python\n",
        "{python}\n",
        "```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "38502585",
      "metadata": {},
      "outputs": [],
      "source": [
        "def messages_for(python):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
        "    ]\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "340bd6fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_output(cpp):\n",
        "    with open(\"main.cpp\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(cpp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c98fee02",
      "metadata": {},
      "outputs": [],
      "source": [
        "def port(client, model, python):\n",
        "    # function that will port the python code to C++\n",
        "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
        "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
        "    reply = response.choices[0].message.content\n",
        "    reply = reply.replace('```cpp','').replace('```','')\n",
        "    write_output(reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3ecf11a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "pi = \"\"\"\n",
        "import time\n",
        "\n",
        "def calculate(iterations, param1, param2):\n",
        "    result = 1.0\n",
        "    for i in range(1, iterations+1):\n",
        "        j = i * param1 - param2\n",
        "        result -= (1/j)\n",
        "        j = i * param1 + param2\n",
        "        result += (1/j)\n",
        "    return result\n",
        "\n",
        "start_time = time.time()\n",
        "result = calculate(200_000_000, 4, 1) * 4\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Result: {result:.12f}\")\n",
        "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7e1d2ab0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_python(code):\n",
        "    #function that will run the python code\n",
        "    globals = {\"__builtins__\": __builtins__}\n",
        "    exec(code, globals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b53e8d22",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: 3.141592656089\n",
            "Execution Time: 23.006779 seconds\n"
          ]
        }
      ],
      "source": [
        "run_python(pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3702568e",
      "metadata": {},
      "outputs": [],
      "source": [
        "port(llm_client, OPENAI_MODEL, pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9d8f9297",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the commands from GPT 5\n",
        "\n",
        "def compile_and_run():\n",
        "    subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
        "    print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)\n",
        "    print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)\n",
        "    print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "fb12277e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: 3.141592662604\n",
            "Execution Time: 0.394998 seconds\n",
            "\n",
            "Result: 3.141592662604\n",
            "Execution Time: 0.393660 seconds\n",
            "\n",
            "Result: 3.141592662604\n",
            "Execution Time: 0.386548 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "compile_and_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6639d9e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trying with GPT 4o\n",
        "port(anthropic, CLAUDE_MODEL, pi)\n",
        "compile_and_run()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
