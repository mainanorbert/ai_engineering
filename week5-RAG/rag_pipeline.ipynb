{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "25fda49d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "from chromadb import PersistentClient\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
        "\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "MODEL = \"claude-haiku-4-5\"\n",
        "DB_NAME = \"preprocessed_db\"\n",
        "collection_name = \"docs\"\n",
        "embedding_model = \"all-MiniLM-L6-v2\"\n",
        "KNOWLEDGE_BASE_PATH = Path(\"knowledge-base\")\n",
        "AVERAGE_CHUNK_SIZE = 500\n",
        "RETRIEVAL_K = 10\n",
        "\n",
        "\n",
        "llm = ChatAnthropic(\n",
        "    model=MODEL,\n",
        "    temperature=0,\n",
        "    max_tokens=5000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bd9d7eae",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Result(BaseModel):\n",
        "    \"\"\"Represents a retrievable chunk plus metadata.\"\"\"\n",
        "\n",
        "    page_content: str\n",
        "    metadata: dict\n",
        "\n",
        "\n",
        "class Chunk(BaseModel):\n",
        "    \"\"\"Represents one generated chunk from an input document.\"\"\"\n",
        "\n",
        "    headline: str = Field(\n",
        "        description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\"\n",
        "    )\n",
        "    summary: str = Field(\n",
        "        description=\"A few sentences summarizing the content of this chunk to answer common questions\"\n",
        "    )\n",
        "    original_text: str = Field(\n",
        "        description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\"\n",
        "    )\n",
        "\n",
        "    def as_result(self, document: dict) -> Result:\n",
        "        \"\"\"Convert a generated chunk into a retrievable result object.\"\"\"\n",
        "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
        "        page_content = f\"{self.headline}\\n\\n{self.summary}\\n\\n{self.original_text}\"\n",
        "        return Result(page_content=page_content, metadata=metadata)\n",
        "\n",
        "\n",
        "class Chunks(BaseModel):\n",
        "    \"\"\"Container model used for structured chunk extraction.\"\"\"\n",
        "\n",
        "    chunks: list[Chunk]\n",
        "\n",
        "\n",
        "class RankOrder(BaseModel):\n",
        "    \"\"\"Represents reranked chunk ids in descending relevance order.\"\"\"\n",
        "\n",
        "    order: list[int] = Field(\n",
        "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "30a21339",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_documents() -> list[dict]:\n",
        "    \"\"\"Load markdown documents from the knowledge base directory.\"\"\"\n",
        "    documents = []\n",
        "\n",
        "    for folder in KNOWLEDGE_BASE_PATH.iterdir():\n",
        "        doc_type = folder.name\n",
        "        for file in folder.rglob(\"*.md\"):\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as file_handle:\n",
        "                documents.append(\n",
        "                    {\n",
        "                        \"type\": doc_type,\n",
        "                        \"source\": file.as_posix(),\n",
        "                        \"text\": file_handle.read(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    print(f\"Loaded {len(documents)} documents\")\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c0a5c03d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_prompt(document: dict) -> str:\n",
        "    \"\"\"Build the prompt used to split one source document into overlapping chunks.\"\"\"\n",
        "    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
        "    return f\"\"\"\n",
        "You take a document and you split the document into overlapping chunks for a KnowledgeBase.\n",
        "\n",
        "The document is from the shared drive of a company called Insurellm.\n",
        "The document is of type: {document[\"type\"]}\n",
        "The document has been retrieved from: {document[\"source\"]}\n",
        "\n",
        "A chatbot will use these chunks to answer questions about the company.\n",
        "You should divide up the document as you see fit, being sure that the entire document is returned in the chunks - don't leave anything out.\n",
        "This document should probably be split into {how_many} chunks, but you can have more or less as appropriate.\n",
        "There should be overlap between the chunks as appropriate; typically about 25% overlap or about 50 words, so you have the same text in multiple chunks for best retrieval results.\n",
        "\n",
        "For each chunk, you should provide a headline, a summary, and the original text of the chunk.\n",
        "Together your chunks should represent the entire document with overlap.\n",
        "\n",
        "Here is the document:\n",
        "\n",
        "{document[\"text\"]}\n",
        "\n",
        "Respond with the chunks.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_messages(document: dict) -> list[dict]:\n",
        "    \"\"\"Create message payload for the chunking LLM call.\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": make_prompt(document)}]\n",
        "\n",
        "\n",
        "def process_document(document: dict) -> list[Result]:\n",
        "    \"\"\"Generate structured chunks for a single document and return retrievable results.\"\"\"\n",
        "    structured_llm = llm.with_structured_output(Chunks)\n",
        "    messages = make_messages(document)\n",
        "    doc_as_chunks_obj = structured_llm.invoke(messages)\n",
        "    return [chunk.as_result(document) for chunk in doc_as_chunks_obj.chunks]\n",
        "\n",
        "\n",
        "def create_chunks(documents: list[dict]) -> list[Result]:\n",
        "    \"\"\"Process all loaded documents into chunk results.\"\"\"\n",
        "    chunks = []\n",
        "    for document in tqdm(documents):\n",
        "        chunks.extend(process_document(document))\n",
        "    return chunks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b73d5529",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 76 documents\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Result(page_content='Company Overview and Structure\\n\\nInsurellm is an innovative insurance tech firm founded in 2015 with 32 employees operating primarily remotely across the US. The company has offices in San Francisco (HQ), New York, Austin, Chicago, and Denver, and has evolved from a high-growth startup to a lean, profitable operation focused on sustainable growth and operational excellence.\\n\\n# Overview of Insurellm\\n\\nInsurellm is an innovative insurance tech firm with 32 employees operating primarily remotely across the US, with offices in San Francisco (HQ), New York, Austin, Chicago, and Denver.\\n\\nFounded in 2015, the company has evolved from a high-growth startup to a lean, profitable operation focused on sustainable growth and operational excellence.', metadata={'source': 'knowledge-base/company/overview.md', 'type': 'company'})]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "documents = fetch_documents()\n",
        "print(create_chunks(documents[:1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "430d85bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_embeddings(chunks: list[Result]) -> None:\n",
        "    \"\"\"Embed chunk texts and store them in a persistent ChromaDB collection.\"\"\"\n",
        "    chroma = PersistentClient(path=DB_NAME)\n",
        "    if collection_name in [collection.name for collection in chroma.list_collections()]:\n",
        "        chroma.delete_collection(collection_name)\n",
        "\n",
        "    texts = [chunk.page_content for chunk in chunks]\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "    vectors = embeddings.embed_documents(texts)\n",
        "\n",
        "    collection = chroma.get_or_create_collection(collection_name)\n",
        "    ids = [str(index) for index in range(len(chunks))]\n",
        "    metadatas = [chunk.metadata for chunk in chunks]\n",
        "\n",
        "    collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metadatas)\n",
        "    print(f\"Vectorstore created with {collection.count()} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1efe0e5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_context_unranked(question: str) -> list[Result]:\n",
        "    \"\"\"Retrieve top-k nearest chunks from ChromaDB before reranking.\"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "    query_vector = embeddings.embed_query(question)\n",
        "\n",
        "    chroma = PersistentClient(path=DB_NAME)\n",
        "    collection = chroma.get_or_create_collection(collection_name)\n",
        "    results = collection.query(query_embeddings=[query_vector], n_results=RETRIEVAL_K)\n",
        "\n",
        "    chunks = []\n",
        "    for document, metadata in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
        "        chunks.append(Result(page_content=document, metadata=metadata))\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def rerank(question: str, chunks: List[Result]) -> List[Result]:\n",
        "    \"\"\"Rerank retrieved chunks by relevance using Claude structured output.\"\"\"\n",
        "    system_prompt = \"\"\"\n",
        "You are a document re-ranker.\n",
        "You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
        "The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance,\n",
        "but you may be able to improve on that.\n",
        "\n",
        "You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
        "\n",
        "Reply ONLY with valid JSON in the following format:\n",
        "{\"order\": [<chunk_id_1>, <chunk_id_2>, ...]}\n",
        "\n",
        "Include all the chunk ids you are provided with, reranked.\n",
        "\"\"\"\n",
        "\n",
        "    user_prompt_lines = [\n",
        "        f\"The user has asked the following question:\\n\\n{question}\\n\",\n",
        "        \"Order all the chunks of text by relevance to the question, from most relevant to least relevant.\",\n",
        "        \"Here are the chunks:\\n\",\n",
        "    ]\n",
        "\n",
        "    for index, chunk in enumerate(chunks, start=1):\n",
        "        user_prompt_lines.append(f\"# CHUNK ID: {index}\\n\\n{chunk.page_content}\\n\")\n",
        "\n",
        "    user_prompt = \"\\n\".join(user_prompt_lines)\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt.strip()),\n",
        "        HumanMessage(content=user_prompt.strip()),\n",
        "    ]\n",
        "\n",
        "    structured_llm = llm.with_structured_output(RankOrder)\n",
        "    rank_order = structured_llm.invoke(messages)\n",
        "    order = rank_order.order\n",
        "    return [chunks[index - 1] for index in order]\n",
        "\n",
        "\n",
        "def fetch_context(question: str) -> list[Result]:\n",
        "    \"\"\"Retrieve and rerank chunks for a user question.\"\"\"\n",
        "    chunks = fetch_context_unranked(question)\n",
        "    return rerank(question, chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "50afd45f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rewrite_query(question: str, history: Optional[List[dict]] = None) -> str:\n",
        "    \"\"\"Rewrite a user question into a focused knowledge-base retrieval query.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    history_str = \"\"\n",
        "    if history:\n",
        "        lines = []\n",
        "        for message in history:\n",
        "            role = \"User\" if message.get(\"role\", \"\").lower() == \"user\" else \"Assistant\"\n",
        "            content = message.get(\"content\", \"\").strip()\n",
        "            if content:\n",
        "                lines.append(f\"{role}: {content}\")\n",
        "        history_str = \"\\n\".join(lines)\n",
        "    else:\n",
        "        history_str = \"(no history)\"\n",
        "\n",
        "    system_content = f\"\"\"\n",
        "You are in a conversation with a user, answering questions about the company Insurellm.\n",
        "You are about to look up information in a Knowledge Base to answer the user's question.\n",
        "\n",
        "This is the history of your conversation so far with the user:\n",
        "{history_str}\n",
        "\n",
        "And this is the user's current question:\n",
        "{question}\n",
        "\n",
        "Respond only with a single, refined question that you will use to search the Knowledge Base.\n",
        "It should be a VERY short specific question most likely to surface content. Focus on the question details.\n",
        "Don't mention the company name unless it's a general question about the company.\n",
        "IMPORTANT: Respond ONLY with the knowledgebase query, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_content),\n",
        "        HumanMessage(content=\"Please rewrite now.\"),\n",
        "    ]\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "    query = response.content.strip()\n",
        "\n",
        "    for bad_start in [\"Query:\", \"Search:\", '\"', \"Refined query:\", \"Here is the query:\"]:\n",
        "        if query.startswith(bad_start):\n",
        "            query = query[len(bad_start):].strip()\n",
        "\n",
        "    return query.strip('\"').strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d9377f70",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
        "You are chatting with a user about Insurellm.\n",
        "Your answer will be evaluated for accuracy, relevance and completeness, so make sure it only answers the question and fully answers it.\n",
        "If you don't know the answer, say so.\n",
        "For context, here are specific extracts from the Knowledge Base that might be directly relevant to the user's question:\n",
        "{context}\n",
        "\n",
        "With this context, please answer the user's question. Be accurate, relevant and complete.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_rag_messages(question: str, history: List[dict], chunks: List[Result]) -> List[dict]:\n",
        "    \"\"\"Create chat messages for final answer generation using retrieved context.\"\"\"\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"Extract from {chunk.metadata['source']}:\\n{chunk.page_content}\" for chunk in chunks\n",
        "    )\n",
        "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
        "    return [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "\n",
        "def answer_question(question: str, history: Optional[List[dict]] = None) -> Tuple[str, List[Result]]:\n",
        "    \"\"\"Run full RAG flow: rewrite, retrieve, rerank, and answer.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    query = rewrite_query(question, history)\n",
        "    chunks = fetch_context(query)\n",
        "    message_dicts = make_rag_messages(question, history, chunks)\n",
        "\n",
        "    messages: List[BaseMessage] = []\n",
        "    for message in message_dicts:\n",
        "        role = message.get(\"role\", \"\").lower()\n",
        "        content = message.get(\"content\", \"\")\n",
        "        if role == \"system\":\n",
        "            messages.append(SystemMessage(content=content))\n",
        "        elif role == \"user\":\n",
        "            messages.append(HumanMessage(content=content))\n",
        "        elif role == \"assistant\":\n",
        "            messages.append(AIMessage(content=content))\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "    answer = response.content.strip()\n",
        "    return answer, chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3345085d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 76 documents\n",
            "[{'type': 'company', 'source': 'knowledge-base/company/overview.md', 'text': \"# Overview of Insurellm\\n\\nInsurellm is an innovative insurance tech firm with 32 employees operating primarily remotely across the US, with offices in San Francisco (HQ), New York, Austin, Chicago, and Denver.\\n\\nFounded in 2015, the company has evolved from a high-growth startup to a lean, profitable operation focused on sustainable growth and operational excellence.\\n\\n## Products\\n\\nInsurellm offers 8 insurance software products across multiple insurance lines:\\n\\n### Core Insurance Portals\\n- **Carllm** - Auto insurance platform for insurers\\n- **Homellm** - Home insurance platform for insurers\\n- **Lifellm** - Life insurance platform with AI-powered underwriting\\n- **Healthllm** - Comprehensive health insurance platform\\n- **Bizllm** - Commercial insurance platform for business coverage\\n\\n### Marketplace & Infrastructure\\n- **Markellm** - Marketplace connecting consumers with insurance providers (original flagship product)\\n- **Claimllm** - AI-powered claims processing platform across all insurance lines\\n- **Rellm** - Enterprise platform for the reinsurance sector\\n\\n## Scale & Impact\\n\\nDespite its compact team size, Insurellm has built an impressive client portfolio with 32 active contracts across all product lines, serving clients ranging from regional insurers to national carriers and global reinsurance partners. The company demonstrates exceptional productivity and leverage through:\\n- Highly automated systems and processes\\n- Strategic use of AI and machine learning\\n- Remote-first culture enabling access to top talent\\n- Focus on high-value enterprise clients in multiple insurance verticals\\n\\n## Client Portfolio Breakdown\\n\\nInsurellm's 32 active contracts span the full spectrum of insurance technology:\\n\\n**Commercial Insurance (Bizllm)**: 7 contracts including regional carriers, multi-line insurers, and national commercial insurance groups\\n\\n**Claims Processing (Claimllm)**: 7 contracts ranging from independent adjusting firms to enterprise claims networks\\n\\n**Life Insurance (Lifellm)**: 6 contracts serving life insurance carriers from small regional providers to major national groups\\n\\n**Health Insurance (Healthllm)**: 6 contracts with health plans from regional insurers to multi-state healthcare alliances\\n\\n**Auto Insurance (Carllm)**: 3 contracts with personal and commercial auto insurers\\n\\n**Home Insurance (Homellm)**: 4 contracts including property insurers with advanced IoT and catastrophe management needs\\n\\n**Insurance Marketplace (Markellm)**: 2 contracts with agencies and brokers leveraging the consumer-insurer matching platform\\n\\n**Reinsurance (Rellm)**: 2 contracts with reinsurance companies including global treaty and facultative operations\\n\\nThis diversified portfolio demonstrates Insurellm's ability to serve the entire insurance value chain, from consumer-facing marketplaces to complex reinsurance operations.\"}, {'type': 'company', 'source': 'knowledge-base/company/about.md', 'text': \"# About Insurellm\\n\\nInsurellm was founded by Avery Lancaster in 2015 as an insurance tech startup designed to disrupt an industry in need of innovative products. Its first product was Markellm, the marketplace connecting consumers with insurance providers.\\n\\nThe company experienced rapid growth in its first five years, expanding its product portfolio to include Carllm (auto insurance portal), Homellm (home insurance portal), and Rellm (enterprise reinsurance platform). By 2020, Insurellm had reached a peak of 200 employees with 12 offices across the US.\\n\\nHowever, the company underwent a strategic restructuring in 2022-2023 to focus on profitability and sustainable growth. This included consolidating office locations, implementing a remote-first strategy, and streamlining operations. As of 2025, Insurellm operates with a lean, highly efficient team of 32 employees who have built a portfolio of 32 active contracts spanning all eight product lines. The company maintains its San Francisco headquarters along with small satellite offices in key markets including New York, Austin, Chicago, and Denver.\\n\\nSince the restructuring, Insurellm has continued to innovate, expanding its product suite to eight comprehensive platforms. The company added Lifellm (life insurance), Healthllm (health insurance), Bizllm (commercial insurance), and Claimllm (claims processing) to serve the full spectrum of insurance needs. This strategic expansion has been highly successful, with strong adoption across all new products:\\n\\n- **Bizllm** quickly gained traction with 7 commercial insurance contracts, including regional carriers and national commercial groups\\n- **Claimllm** signed 7 contracts ranging from independent adjusting firms to enterprise claims networks\\n- **Lifellm** secured 6 life insurance clients from small regional providers to major national carriers\\n- **Healthllm** won 6 health plan contracts including regional insurers and multi-state healthcare alliances\\n\\nCombined with continued growth in the original product lines (Carllm, Homellm, Markellm, and Rellm), Insurellm now serves clients ranging from regional insurers to global reinsurance partners, demonstrating the company's ability to compete across the entire insurance value chain.\"}, {'type': 'company', 'source': 'knowledge-base/company/culture.md', 'text': \"# Insurellm Culture\\n\\n## Vision Statement\\nTo revolutionize the insurance industry through innovative technology that makes insurance accessible, transparent, and effortless for everyone.\\n\\n## Mission Statement\\nWe empower insurance providers and consumers with cutting-edge software solutions that streamline processes, enhance customer experiences, and drive meaningful connections in the insurance marketplace. By combining deep industry expertise with technological innovation, we're building the future of insurance.\\n\\n## Core Values\\n\\n### Innovation First\\nWe challenge the status quo and embrace creative problem-solving. Our team is encouraged to experiment, take calculated risks, and push the boundaries of what's possible in insurance technology. We believe that breakthrough solutions come from curiosity, collaboration, and a willingness to learn from both successes and failures.\\n\\n### Customer Obsession\\nOur clients' success is our success. We deeply understand our customers' needs and work tirelessly to exceed their expectations. Every product decision, every feature, and every interaction is guided by a commitment to delivering exceptional value and building lasting partnerships.\\n\\n### Integrity & Transparency\\nWe operate with honesty and openness in everything we do. From our products to our internal operations, we believe that trust is earned through consistent ethical behavior, clear communication, and accountability at all levels of the organization.\\n\\n### Collaborative Excellence\\nWe achieve more together. We foster a culture where diverse perspectives are valued, knowledge is shared freely, and every team member is empowered to contribute their unique talents. We celebrate collective wins and support each other through challenges.\\n\\n## Employer Value Proposition\\n\\n**Build the Future of Insurance Technology with Elite Talent**\\n\\nAt Insurellm, you'll join an elite team of 32 exceptional professionals transforming a traditional industry with cutting-edge technology. After our strategic restructuring in 2022-2023, we evolved from a 200-person startup to a lean, high-performing organization where every person makes a significant impact. With 32 active contracts across all eight product lines—from regional insurers to global reinsurance partners—each team member directly influences technology used by thousands of insurance professionals and millions of consumers. We offer:\\n\\n- **Meaningful Impact**: Your work directly shapes products powering 32 active insurance operations, from commercial carriers to health plans to global reinsurance firms\\n- **Growth & Development**: Comprehensive professional development programs, mentorship opportunities, and clear career progression paths\\n- **Innovation Freedom**: Access to the latest technologies and the autonomy to experiment with new ideas\\n- **Work-Life Harmony**: Flexible working arrangements, generous PTO, and a culture that respects personal time\\n- **Competitive Rewards**: Market-leading compensation, equity participation, comprehensive benefits, and performance bonuses\\n- **Inclusive Environment**: A diverse team where every voice matters and different perspectives drive better solutions\\n- **Purpose-Driven Mission**: Be part of making insurance more accessible and fair for everyone\\n\\nJoin us in disrupting an industry and building technology that matters.\\n\"}, {'type': 'company', 'source': 'knowledge-base/company/careers.md', 'text': \"# Careers at Insurellm\\n\\n## Why Join Insurellm?\\n\\nAt Insurellm, we're not just building software—we're revolutionizing an entire industry. Since our founding in 2015, we've evolved from a high-growth startup to a lean, profitable company with 32 highly talented employees managing 32 active contracts across all eight of our product lines.\\n\\nAfter reaching 200 employees in 2020, we strategically restructured in 2022-2023 to focus on sustainable growth, operational excellence, and building a world-class remote-first culture. Today, we're a tight-knit team of exceptional professionals who deliver outsized impact through automation, AI, and strategic focus on high-value enterprise clients—from regional insurers to global reinsurance partners.\\n\\n### Our Culture\\n\\nWe live by our core values every day:\\n- **Innovation First**: We encourage experimentation and creative problem-solving\\n- **Customer Obsession**: Your work directly impacts 32 active client operations spanning the entire insurance value chain\\n- **Integrity & Transparency**: We build trust through ethical behavior and open communication\\n- **Collaborative Excellence**: Diverse perspectives and teamwork drive our success\\n\\n### What We Offer\\n\\n- Competitive compensation with equity participation\\n- Comprehensive health, dental, and vision insurance\\n- Flexible working arrangements and generous PTO\\n- Professional development programs and mentorship\\n- Clear career progression paths\\n- Latest technologies and tools\\n- Inclusive, diverse work environment\\n\\n## Current Opportunities\\n\\n### Engineering\\n\\n**Senior Full Stack Engineer** - San Francisco, CA\\n- Lead development of next-generation insurance platform features\\n- Work with React, Node.js, Python, and cloud technologies\\n- Mentor junior engineers and drive technical decisions\\n- 5+ years experience required\\n\\n**Backend Software Engineer** - San Francisco, CA / Austin, TX\\n- Build scalable microservices and APIs\\n- Optimize system performance and reliability\\n- Collaborate with product and design teams\\n- 3+ years experience with Java, Python, or Go\\n\\n**Frontend Developer** - Remote\\n- Create intuitive user interfaces for our insurance platforms\\n- Work with modern frameworks (React, Vue, or Angular)\\n- Ensure accessibility and responsive design\\n- 2+ years experience required\\n\\n**DevOps Engineer** - New York, NY\\n- Manage cloud infrastructure (AWS/GCP)\\n- Implement CI/CD pipelines and automation\\n- Monitor system performance and security\\n- 3+ years experience in DevOps/SRE\\n\\n**Mobile Developer (iOS/Android)** - San Francisco, CA\\n- Build native mobile applications for our marketplace\\n- Create seamless user experiences\\n- Integrate with backend APIs\\n- 3+ years mobile development experience\\n\\n### Data & Analytics\\n\\n**Senior Data Scientist** - San Francisco, CA / New York, NY\\n- Develop predictive models for insurance risk assessment\\n- Build recommendation systems for our marketplace\\n- Lead data-driven product initiatives\\n- PhD or MS in related field preferred, 4+ years experience\\n\\n**Data Engineer** - Austin, TX\\n- Design and maintain data pipelines\\n- Build data warehousing solutions\\n- Optimize data infrastructure for scale\\n- 3+ years experience with SQL, Python, and ETL tools\\n\\n**Business Intelligence Analyst** - Chicago, IL\\n- Create dashboards and reports for stakeholders\\n- Analyze business metrics and trends\\n- Support data-driven decision making\\n- 2+ years experience with BI tools (Tableau, Looker, etc.)\\n\\n### Product & Design\\n\\n**Product Manager** - San Francisco, CA\\n- Define product roadmap and strategy\\n- Work closely with engineering and design teams\\n- Gather customer insights and market research\\n- 3+ years product management experience in B2B SaaS\\n\\n**UX/UI Designer** - Remote\\n- Design user experiences for our insurance platforms\\n- Conduct user research and usability testing\\n- Create design systems and prototypes\\n- 3+ years design experience, insurance/fintech preferred\\n\\n### Sales & Customer Success\\n\\n**Account Executive** - New York, NY / Chicago, IL / Austin, TX\\n- Manage B2B sales cycle for enterprise clients\\n- Build relationships with insurance companies\\n- Exceed revenue targets and grow territory\\n- 3+ years B2B sales experience, SaaS preferred\\n\\n**Sales Development Representative** - Austin, TX / Remote\\n- Generate qualified leads for sales team\\n- Conduct outreach to prospective clients\\n- Research and identify target accounts\\n- 1+ years SDR/BDR experience preferred\\n\\n**Customer Success Manager** - San Francisco, CA / New York, NY\\n- Ensure client satisfaction and retention\\n- Drive product adoption and expansion\\n- Serve as trusted advisor to clients\\n- 2+ years customer success experience in SaaS\\n\\n**Solutions Engineer** - Remote\\n- Provide technical expertise during sales process\\n- Conduct product demonstrations and POCs\\n- Support implementation and integration\\n- 3+ years technical pre-sales experience\\n\\n### Operations & Support\\n\\n**Technical Support Specialist** - Remote\\n- Provide tier 2/3 technical support to clients\\n- Troubleshoot platform issues\\n- Create documentation and knowledge base articles\\n- 2+ years technical support experience\\n\\n**HR Business Partner** - San Francisco, CA\\n- Partner with leadership on people strategy\\n- Support talent development and retention\\n- Drive culture and engagement initiatives\\n- 4+ years HR experience, tech industry preferred\\n\\n## How to Apply\\n\\nVisit our careers portal at careers.insurellm.com or send your resume to jobs@insurellm.com. Please include the position title in your subject line.\\n\\nInsurellm is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\"}, {'type': 'contracts', 'source': 'knowledge-base/contracts/Contract with Rapid Claims Associates for Claimllm.md', 'text': \"# Contract with Rapid Claims Associates for Claimllm\\n\\n**Contract Date:** March 1, 2025\\n**Contract Number:** CL-2025-0063\\n**Parties:**\\n- Insurellm, Inc.\\n- Rapid Claims Associates, LLC\\n\\n---\\n\\n## Terms\\n\\n1. **Coverage:** Insurellm agrees to provide Rapid Claims Associates with access to the Claimllm platform, enabling AI-powered claims processing, automated triage, and streamlined claims management across property, casualty, and auto lines.\\n\\n2. **Duration:** This agreement is effective for a period of 12 months from the contract date, with automatic renewal provisions unless terminated with 30-day written notice.\\n\\n3. **Payment:** Rapid Claims Associates shall pay a monthly fee of $4,500, due by the 1st of every month for the Core Tier package, supporting up to 5,000 claims annually.\\n\\n4. **Overage Fees:** If claims volume exceeds 5,000 annually, overage fees of $0.90 per claim apply, billed quarterly.\\n\\n5. **Confidentiality:** Both parties agree to maintain confidentiality of proprietary information, claimant data, and processing methodologies disclosed during this contract.\\n\\n6. **Data Ownership:** Rapid Claims Associates retains ownership of all claims data. Insurellm may use anonymized, aggregated data for product improvement.\\n\\n7. **Liability:** Insurellm's liability is limited to direct damages not exceeding total fees paid in the preceding 6 months.\\n\\n---\\n\\n## Renewal\\n\\nUnless either party provides written notice of termination at least 30 days prior to contract expiration, this agreement automatically renews for successive 12-month terms. Pricing may be adjusted annually with 60 days' advance notice, capped at 10% increase per year.\\n\\n---\\n\\n## Features\\n\\nRapid Claims Associates will receive the following Core Tier features:\\n\\n1. **Intelligent FNOL Processing:** Multi-channel first notice of loss intake via:\\n   - Mobile app integration\\n   - Web portal submission\\n   - Email parsing with NLP extraction\\n   - Phone system integration with IVR\\n   - Chatbot-assisted intake\\n\\n2. **Automated Triage and Routing:** Machine learning algorithms assess claim severity and complexity to route appropriately:\\n   - Low-complexity claims → automated processing\\n   - Medium-complexity claims → junior adjuster queue\\n   - High-complexity/high-value claims → senior adjuster assignment\\n   - Fraud-flagged claims → special investigation unit\\n\\n3. **Basic Document Processing:** OCR and NLP extraction from:\\n   - Police reports\\n   - Repair estimates\\n   - Medical bills and records\\n   - Witness statements\\n   - Photo and video evidence\\n\\n4. **Standard Fraud Detection:** Rule-based fraud screening including:\\n   - Duplicate claim detection\\n   - Unusual pattern identification\\n   - Loss location verification\\n   - Claimant history review\\n\\n5. **Claimant Communication Hub:** Automated status updates via:\\n   - SMS text messages\\n   - Email notifications\\n   - Mobile app push notifications\\n   - Scheduled status update intervals\\n\\n6. **Basic Reporting:** Standard dashboards tracking:\\n   - Claims volume and cycle time\\n   - Average settlement amounts\\n   - Adjuster productivity metrics\\n   - Customer satisfaction scores\\n\\n7. **Payment Processing:** Integration with payment systems for:\\n   - Direct deposit (ACH)\\n   - Check issuance\\n   - Digital wallets (PayPal, Venmo)\\n   - Simple multi-party splits\\n\\n---\\n\\n## Support\\n\\nInsurellm commits to providing comprehensive support to Rapid Claims Associates:\\n\\n1. **Onboarding:** 2-week implementation program including:\\n   - System configuration and data migration\\n   - Training for up to 10 claims staff members (12 hours total)\\n   - Process workflow mapping and optimization\\n   - Integration with existing claims management system\\n\\n2. **Technical Support:**\\n   - Email and phone support Monday-Friday 8 AM - 6 PM EST\\n   - Response time: 8 hours for critical issues, 24 hours for standard requests\\n   - Online knowledge base and FAQ access\\n   - Monthly platform office hours for questions\\n\\n3. **Platform Updates:**\\n   - Quarterly feature releases with new capabilities\\n   - Monthly security patches and bug fixes\\n   - Release notes and changelog documentation\\n   - Advance notice of breaking changes (minimum 30 days)\\n\\n4. **Account Management:**\\n   - Named customer success manager\\n   - Semi-annual business review meetings\\n   - Usage analytics and optimization recommendations\\n   - Assistance with tier upgrade evaluation when volume grows\\n\\n5. **Integration Support:** Technical assistance connecting Claimllm with:\\n   - Existing claims management systems\\n   - Policy administration platforms\\n   - Vendor management networks (repair shops, medical providers)\\n   - Payment processing systems\\n\\n---\\n\\n**Signatures:**\\n\\n_________________________________\\n**Sarah Chen**\\n**Title**: Vice President of Sales\\n**Insurellm, Inc.**\\n**Date**: March 1, 2025\\n\\n_________________________________\\n**Marcus Johnson**\\n**Title**: Chief Claims Officer\\n**Rapid Claims Associates, LLC**\\n**Date**: March 1, 2025\\n\\n---\\n\\nThis agreement represents the complete understanding between Insurellm and Rapid Claims Associates regarding the Claimllm platform and supersedes any prior communications or agreements.\\n\"}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:48<00:00,  9.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorstore created with 41 documents\n",
            "\n",
            "Question: Tell me about the Insurellm?\n",
            "Answer: # About Insurellm\n",
            "\n",
            "Insurellm is an innovative insurance technology company that's revolutionizing the insurance industry through cutting-edge software solutions.\n",
            "\n",
            "## Company Overview\n",
            "\n",
            "**Founded:** 2015 by Avery Lancaster\n",
            "\n",
            "**Current Size:** 32 employees operating primarily remotely across the US\n",
            "\n",
            "**Headquarters:** San Francisco, with satellite offices in New York, Austin, Chicago, and Denver\n",
            "\n",
            "## Vision & Mission\n",
            "\n",
            "**Vision:** To revolutionize the insurance industry through innovative technology that makes insurance accessible, transparent, and effortless for everyone.\n",
            "\n",
            "**Mission:** We empower insurance providers and consumers with cutting-edge software solutions that streamline processes, enhance customer experiences, and drive meaningful connections in the insurance marketplace.\n",
            "\n",
            "## Products & Services\n",
            "\n",
            "Insurellm offers **8 comprehensive insurance software products** across multiple insurance lines:\n",
            "\n",
            "- **Carllm** - Auto insurance platform\n",
            "- **Homellm** - Home insurance platform\n",
            "- **Lifellm** - Life insurance with AI-powered underwriting\n",
            "- **Healthllm** - Health insurance platform\n",
            "- **Bizllm** - Commercial insurance platform\n",
            "- **Claimllm** - Claims processing platform\n",
            "- **Markellm** - Insurance marketplace (original product)\n",
            "- **Rellm** - Enterprise reinsurance platform\n",
            "\n",
            "## Company Evolution\n",
            "\n",
            "Insurellm grew rapidly from 2015-2020, reaching 200 employees across 12 offices. In 2022-2023, the company underwent strategic restructuring to focus on profitability and sustainable growth, becoming a lean, highly efficient operation with 32 active contracts serving regional insurers, health plans, and global reinsurance partners.\n",
            "\n",
            "\n",
            "Question: Who went to Manchester University?\n",
            "Answer: I don't have information about which employees at Insurellm attended Manchester University. The knowledge base provided doesn't contain details about individual employees' educational backgrounds or universities they attended.\n",
            "\n",
            "If you're looking for information about a specific person at Insurellm, I'd recommend reaching out to the company directly through their careers portal at careers.insurellm.com or contacting jobs@insurellm.com.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def run_pipeline_demo() -> None:\n",
        "    \"\"\"Build the knowledge base and run two example RAG questions.\"\"\"\n",
        "    documents = fetch_documents()\n",
        "    print(documents[:5])\n",
        "    chunks = create_chunks(documents[:5])\n",
        "    create_embeddings(chunks)\n",
        "\n",
        "    demo_questions = [\n",
        "        \"Tell me about the Insurellm?\",\n",
        "        \"Who went to Manchester University?\",\n",
        "    ]\n",
        "\n",
        "    for question in demo_questions:\n",
        "        answer, _ = answer_question(question, [])\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "        print(f\"Answer: {answer}\\n\")\n",
        "\n",
        "\n",
        "run_pipeline_demo()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
