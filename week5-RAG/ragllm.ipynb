{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "357ca264",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Optional\n",
        "\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "from chromadb import PersistentClient\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, BaseMessage\n",
        "\n",
        "from transcriber import transcriber\n",
        "\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "MODEL = \"claude-haiku-4-5\"\n",
        "DB_NAME = \"preprocessed_db\"\n",
        "COLLECTION_NAME = \"docs\"\n",
        "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "RETRIEVAL_K = 8\n",
        "MAX_KB_DISTANCE = 1.1\n",
        "\n",
        "\n",
        "llm = ChatAnthropic(model=MODEL, temperature=0.0, max_tokens=1800)\n",
        "embedding_client = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "web_search_tool = DuckDuckGoSearchRun()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3478dafd",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Result(BaseModel):\n",
        "    \"\"\"Represents a retrievable chunk plus metadata.\"\"\"\n",
        "\n",
        "    page_content: str\n",
        "    metadata: dict\n",
        "\n",
        "\n",
        "class RankOrder(BaseModel):\n",
        "    \"\"\"Represents reranked chunk ids in descending relevance order.\"\"\"\n",
        "\n",
        "    order: List[int] = Field(\n",
        "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "809ce221",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_collection_or_raise():\n",
        "    \"\"\"Return the pre-built vector collection or raise a clear error.\"\"\"\n",
        "    chroma = PersistentClient(path=DB_NAME)\n",
        "    collection_names = [collection.name for collection in chroma.list_collections()]\n",
        "    if COLLECTION_NAME not in collection_names:\n",
        "        raise ValueError(\n",
        "            f\"Collection '{COLLECTION_NAME}' was not found in '{DB_NAME}'. Build vectors first.\"\n",
        "        )\n",
        "\n",
        "    collection = chroma.get_or_create_collection(COLLECTION_NAME)\n",
        "    if collection.count() == 0:\n",
        "        raise ValueError(\n",
        "            f\"Collection '{COLLECTION_NAME}' is empty in '{DB_NAME}'. Build vectors first.\"\n",
        "        )\n",
        "    return collection\n",
        "\n",
        "\n",
        "def rerank_chunks(question: str, chunks: List[Result]) -> List[Result]:\n",
        "    \"\"\"Rerank retrieved chunks by relevance using Claude structured output.\"\"\"\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    prompt_lines = [\n",
        "        f\"Question:\\n{question}\\n\",\n",
        "        \"Chunks:\\n\",\n",
        "    ]\n",
        "    for index, chunk in enumerate(chunks, start=1):\n",
        "        prompt_lines.append(f\"# CHUNK ID: {index}\\n\\n{chunk.page_content}\\n\")\n",
        "\n",
        "    structured_llm = llm.with_structured_output(RankOrder)\n",
        "    rank_order = structured_llm.invoke(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=(\n",
        "                    \"You are a document re-ranker. Rank chunk ids by relevance and return only JSON: \"\n",
        "                    \"{\\\"order\\\": [<id_1>, <id_2>, ...]} including all chunk ids once.\"\n",
        "                )\n",
        "            ),\n",
        "            HumanMessage(content=\"\\n\".join(prompt_lines).strip()),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    valid_order = [index for index in rank_order.order if 1 <= index <= len(chunks)]\n",
        "    if not valid_order:\n",
        "        return chunks\n",
        "    return [chunks[index - 1] for index in valid_order]\n",
        "\n",
        "\n",
        "def format_rag_context(question: str) -> str:\n",
        "    \"\"\"Retrieve and rerank context from ChromaDB, then format for answering.\"\"\"\n",
        "    collection = get_collection_or_raise()\n",
        "    query_vector = embedding_client.embed_query(question)\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_vector],\n",
        "        n_results=RETRIEVAL_K,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
        "    )\n",
        "\n",
        "    documents = results.get(\"documents\", [[]])[0]\n",
        "    metadatas = results.get(\"metadatas\", [[]])[0]\n",
        "    distances = results.get(\"distances\", [[]])[0]\n",
        "\n",
        "    chunks = [\n",
        "        Result(page_content=document, metadata=metadata)\n",
        "        for document, metadata in zip(documents, metadatas)\n",
        "    ]\n",
        "    chunks = rerank_chunks(question, chunks)\n",
        "\n",
        "    if not chunks:\n",
        "        return \"NO_RELEVANT_KB_CONTEXT\"\n",
        "\n",
        "    if distances and min(distances) > MAX_KB_DISTANCE:\n",
        "        return \"NO_RELEVANT_KB_CONTEXT\"\n",
        "\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"Extract from {chunk.metadata.get('source', 'unknown')}:\\n{chunk.page_content}\"\n",
        "        for chunk in chunks\n",
        "    )\n",
        "    return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e078470e",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def rag_search(query: str) -> str:\n",
        "    \"\"\"Search the Insurellm knowledge base and return relevant context text.\"\"\"\n",
        "    context = format_rag_context(query)\n",
        "    return context\n",
        "\n",
        "\n",
        "@tool\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"Search the public web for general information when KB context is missing.\"\"\"\n",
        "    return web_search_tool.run(query)\n",
        "\n",
        "\n",
        "@tool\n",
        "def transcribe_audio(path_to_file: str) -> str:\n",
        "    \"\"\"Transcribe a local audio file into plain text.\"\"\"\n",
        "    return transcriber(path_to_file)\n",
        "\n",
        "\n",
        "tools = [rag_search, web_search, transcribe_audio]\n",
        "agent_llm = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def handle_tool_call(tool_call: dict) -> ToolMessage:\n",
        "    \"\"\"Execute one tool call and convert result into a LangChain ToolMessage.\"\"\"\n",
        "    tool_name = tool_call.get(\"name\", \"\")\n",
        "    tool_args = tool_call.get(\"args\", {})\n",
        "    tool_call_id = tool_call.get(\"id\", \"\")\n",
        "\n",
        "    if tool_name == \"rag_search\":\n",
        "        content = rag_search.invoke(tool_args)\n",
        "    elif tool_name == \"web_search\":\n",
        "        content = web_search.invoke(tool_args)\n",
        "    elif tool_name == \"transcribe_audio\":\n",
        "        content = transcribe_audio.invoke(tool_args)\n",
        "    else:\n",
        "        content = f\"Unknown tool: {tool_name}\"\n",
        "\n",
        "    return ToolMessage(content=content, tool_call_id=tool_call_id)\n",
        "\n",
        "\n",
        "def convert_history_to_messages(history: List[dict]) -> List[BaseMessage]:\n",
        "    \"\"\"Convert Gradio message history into LangChain message objects.\"\"\"\n",
        "    messages: List[BaseMessage] = []\n",
        "    for item in history:\n",
        "        role = item.get(\"role\", \"\")\n",
        "        content = str(item.get(\"content\", \"\"))\n",
        "        if role == \"user\":\n",
        "            messages.append(HumanMessage(content=content))\n",
        "        elif role == \"assistant\":\n",
        "            messages.append(AIMessage(content=content))\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6ae8f34f",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an Insurellm assistant that must use tools when needed.\n",
        "\n",
        "Rules:\n",
        "1) For company questions, call `rag_search` first.\n",
        "2) If `rag_search` returns `NO_RELEVANT_KB_CONTEXT`, call `web_search`.\n",
        "3) When using web fallback, be transparent and begin with:\n",
        "   \"There is no such information in our Insurellm knowledge base. Here is general information from the internet:\"\n",
        "4) If the user provides an audio file path, call `transcribe_audio` first.\n",
        "5) Be concise, accurate, and user-friendly.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def run_agent(user_text: str, history: Optional[List[dict]] = None) -> str:\n",
        "    \"\"\"Run a tool-calling conversation loop and return the final assistant text.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    used_web_fallback = False\n",
        "    messages: List[BaseMessage] = [SystemMessage(content=SYSTEM_PROMPT.strip())]\n",
        "    messages.extend(convert_history_to_messages(history))\n",
        "    messages.append(HumanMessage(content=user_text))\n",
        "\n",
        "    for _ in range(4):\n",
        "        ai_message = agent_llm.invoke(messages)\n",
        "        messages.append(ai_message)\n",
        "\n",
        "        if not ai_message.tool_calls:\n",
        "            final_text = ai_message.content if isinstance(ai_message.content, str) else str(ai_message.content)\n",
        "            if used_web_fallback:\n",
        "                prefix = (\n",
        "                    \"There is no such information in our Insurellm knowledge base. \"\n",
        "                    \"Here is general information from the internet:\\n\\n\"\n",
        "                )\n",
        "                if not final_text.startswith(\"There is no such information in our Insurellm knowledge base\"):\n",
        "                    final_text = prefix + final_text\n",
        "            return final_text\n",
        "\n",
        "        for tool_call in ai_message.tool_calls:\n",
        "            if tool_call.get(\"name\") == \"web_search\":\n",
        "                used_web_fallback = True\n",
        "            tool_message = handle_tool_call(tool_call)\n",
        "            messages.append(tool_message)\n",
        "\n",
        "    return \"I could not complete the request after several tool steps. Please rephrase your question.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4e60997d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_vector_store_status() -> str:\n",
        "    \"\"\"Return a status string for the preprocessed vector store.\"\"\"\n",
        "    try:\n",
        "        collection = get_collection_or_raise()\n",
        "        return f\"Vector store ready: {collection.count()} chunks in '{COLLECTION_NAME}' from '{DB_NAME}'.\"\n",
        "    except Exception as exc:\n",
        "        return f\"Vector store error: {exc}\"\n",
        "\n",
        "\n",
        "def create_audio_tool_call(path_to_file: str) -> dict:\n",
        "    \"\"\"Create a synthetic tool call object for transcribing user audio.\"\"\"\n",
        "    return {\n",
        "        \"name\": \"transcribe_audio\",\n",
        "        \"args\": {\"path_to_file\": path_to_file},\n",
        "        \"id\": \"manual_audio_transcribe\",\n",
        "    }\n",
        "\n",
        "\n",
        "def chat_handler(user_text: str, audio_path: str, history: List[dict]):\n",
        "    \"\"\"Handle UI requests, transcribe audio filepath if present, and answer with the agent.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    final_user_text = (user_text or \"\").strip()\n",
        "\n",
        "    if audio_path:\n",
        "        transcription_tool_message = handle_tool_call(create_audio_tool_call(audio_path))\n",
        "        transcribed_text = str(transcription_tool_message.content).strip()\n",
        "        if final_user_text:\n",
        "            final_user_text = f\"{final_user_text}\\n\\nTranscribed audio: {transcribed_text}\"\n",
        "        else:\n",
        "            final_user_text = transcribed_text\n",
        "\n",
        "    if not final_user_text:\n",
        "        return history, \"\", None\n",
        "\n",
        "    response_text = run_agent(final_user_text, history)\n",
        "\n",
        "    updated_history = history + [\n",
        "        {\"role\": \"user\", \"content\": final_user_text},\n",
        "        {\"role\": \"assistant\", \"content\": response_text},\n",
        "    ]\n",
        "    return updated_history, \"\", None\n",
        "\n",
        "\n",
        "def build_gradio_app() -> gr.Blocks:\n",
        "    \"\"\"Create a simple Gradio interface for the RAG tool-calling agent.\"\"\"\n",
        "    with gr.Blocks(title=\"Insurellm RAG Agent\") as demo:\n",
        "        gr.Markdown(\"# Insurellm RAG Agent\")\n",
        "        gr.Markdown(\n",
        "            \"Uses preprocessed vectors from ChromaDB, falls back to web search when KB is missing details, and supports microphone/upload transcription.\"\n",
        "        )\n",
        "\n",
        "        status_box = gr.Textbox(label=\"Vector Store Status\", interactive=False)\n",
        "        refresh_status_button = gr.Button(\"Refresh Vector Store Status\")\n",
        "\n",
        "        chatbot = gr.Chatbot(label=\"Assistant\")\n",
        "        user_text = gr.Textbox(label=\"Message\", placeholder=\"Ask about Insurellm...\")\n",
        "        audio_input = gr.Audio(\n",
        "            label=\"Optional audio question\",\n",
        "            sources=[\"microphone\"],\n",
        "            type=\"filepath\",\n",
        "        )\n",
        "        send_button = gr.Button(\"Send\")\n",
        "\n",
        "        send_button.click(\n",
        "            fn=chat_handler,\n",
        "            inputs=[user_text, audio_input, chatbot],\n",
        "            outputs=[chatbot, user_text, audio_input],\n",
        "        )\n",
        "\n",
        "        refresh_status_button.click(fn=check_vector_store_status, outputs=status_box)\n",
        "        demo.load(fn=check_vector_store_status, outputs=status_box)\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "demo_app = build_gradio_app()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc4fcd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_app.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
