{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "357ca264",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "from chromadb import PersistentClient\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "MODEL = \"claude-haiku-4-5\"\n",
        "DB_NAME = \"preprocessed_db\"\n",
        "COLLECTION_NAME = \"docs\"\n",
        "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "RETRIEVAL_K = 8\n",
        "MAX_KB_DISTANCE = 1.1\n",
        "\n",
        "\n",
        "llm = ChatAnthropic(model=MODEL, temperature=0.0, max_tokens=1800)\n",
        "embedding_client = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "web_search_tool = DuckDuckGoSearchRun()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "809ce221",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Result(BaseModel):\n",
        "    \"\"\"Represents a retrievable chunk plus metadata.\"\"\"\n",
        "\n",
        "    page_content: str\n",
        "    metadata: dict\n",
        "\n",
        "\n",
        "class RankOrder(BaseModel):\n",
        "    \"\"\"Represents reranked chunk ids in descending relevance order.\"\"\"\n",
        "\n",
        "    order: List[int] = Field(\n",
        "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
        "    )\n",
        "\n",
        "\n",
        "class RelevanceDecision(BaseModel):\n",
        "    \"\"\"Represents whether KB context can answer the user question.\"\"\"\n",
        "\n",
        "    has_relevant_context: bool = Field(\n",
        "        description=\"True when the context has enough information to answer accurately\"\n",
        "    )\n",
        "    rationale: str = Field(description=\"Short explanation for the decision\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e078470e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_message_content(content: object) -> str:\n",
        "    \"\"\"Normalize chat content values into plain text.\"\"\"\n",
        "    if isinstance(content, str):\n",
        "        return content.strip()\n",
        "\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for item in content:\n",
        "            if isinstance(item, str):\n",
        "                text = item.strip()\n",
        "                if text:\n",
        "                    parts.append(text)\n",
        "            elif isinstance(item, dict):\n",
        "                text = str(item.get(\"text\", \"\")).strip()\n",
        "                if text:\n",
        "                    parts.append(text)\n",
        "        return \"\\n\".join(parts).strip()\n",
        "\n",
        "    if isinstance(content, dict):\n",
        "        return str(content.get(\"text\", \"\")).strip()\n",
        "\n",
        "    return str(content).strip()\n",
        "\n",
        "\n",
        "def get_collection_or_raise():\n",
        "    \"\"\"Return the pre-built vector collection or raise a helpful error.\"\"\"\n",
        "    chroma = PersistentClient(path=DB_NAME)\n",
        "    collection_names = [collection.name for collection in chroma.list_collections()]\n",
        "    if COLLECTION_NAME not in collection_names:\n",
        "        raise ValueError(\n",
        "            f\"Collection '{COLLECTION_NAME}' not found in {DB_NAME}. Build the vector DB first.\"\n",
        "        )\n",
        "\n",
        "    collection = chroma.get_or_create_collection(COLLECTION_NAME)\n",
        "    if collection.count() == 0:\n",
        "        raise ValueError(\n",
        "            f\"Collection '{COLLECTION_NAME}' is empty in {DB_NAME}. Build embeddings first.\"\n",
        "        )\n",
        "    return collection\n",
        "\n",
        "\n",
        "def rewrite_query(question: str, history: Optional[List[dict]] = None) -> str:\n",
        "    \"\"\"Rewrite a user question into a concise retrieval query.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    history_str = \"\"\n",
        "    if history:\n",
        "        lines = []\n",
        "        for message in history:\n",
        "            role = \"User\" if message.get(\"role\", \"\").lower() == \"user\" else \"Assistant\"\n",
        "            content = normalize_message_content(message.get(\"content\", \"\"))\n",
        "            if content:\n",
        "                lines.append(f\"{role}: {content}\")\n",
        "        history_str = \"\\n\".join(lines)\n",
        "    else:\n",
        "        history_str = \"(no history)\"\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "You are in a conversation with a user about Insurellm.\n",
        "Rewrite the current user question into one short, specific search query for a knowledge base.\n",
        "\n",
        "Conversation history:\n",
        "{history_str}\n",
        "\n",
        "Current question:\n",
        "{question}\n",
        "\n",
        "Respond with the rewritten query only.\n",
        "\"\"\"\n",
        "\n",
        "    response = llm.invoke(\n",
        "        [\n",
        "            SystemMessage(content=system_prompt.strip()),\n",
        "            HumanMessage(content=\"Rewrite now.\"),\n",
        "        ]\n",
        "    )\n",
        "    return response.content.strip().strip('\"')\n",
        "\n",
        "\n",
        "def fetch_context_unranked(question: str) -> tuple[List[Result], List[float]]:\n",
        "    \"\"\"Retrieve nearest chunks from preprocessed ChromaDB without reranking.\"\"\"\n",
        "    collection = get_collection_or_raise()\n",
        "    query_vector = embedding_client.embed_query(question)\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_vector],\n",
        "        n_results=RETRIEVAL_K,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
        "    )\n",
        "\n",
        "    chunks = []\n",
        "    distances = results.get(\"distances\", [[]])[0]\n",
        "    for document, metadata in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
        "        chunks.append(Result(page_content=document, metadata=metadata))\n",
        "    return chunks, distances\n",
        "\n",
        "\n",
        "def rerank(question: str, chunks: List[Result]) -> List[Result]:\n",
        "    \"\"\"Rerank retrieved chunks by relevance using structured output.\"\"\"\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "You are a document re-ranker.\n",
        "Rank chunk IDs by relevance to the question.\n",
        "Reply ONLY as JSON: {\"order\": [<id_1>, <id_2>, ...]}.\n",
        "Include all IDs exactly once.\n",
        "\"\"\"\n",
        "\n",
        "    prompt_lines = [\n",
        "        f\"Question:\\n{question}\\n\",\n",
        "        \"Chunks:\\n\",\n",
        "    ]\n",
        "    for index, chunk in enumerate(chunks, start=1):\n",
        "        prompt_lines.append(f\"# CHUNK ID: {index}\\n{chunk.page_content}\\n\")\n",
        "\n",
        "    structured_llm = llm.with_structured_output(RankOrder)\n",
        "    response_obj = structured_llm.invoke(\n",
        "        [\n",
        "            SystemMessage(content=system_prompt.strip()),\n",
        "            HumanMessage(content=\"\\n\".join(prompt_lines).strip()),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    valid_order = [index for index in response_obj.order if 1 <= index <= len(chunks)]\n",
        "    if not valid_order:\n",
        "        return chunks\n",
        "    return [chunks[index - 1] for index in valid_order]\n",
        "\n",
        "\n",
        "def fetch_context(question: str) -> tuple[List[Result], List[float]]:\n",
        "    \"\"\"Retrieve and rerank chunks from preprocessed vector store.\"\"\"\n",
        "    chunks, distances = fetch_context_unranked(question)\n",
        "    reranked_chunks = rerank(question, chunks)\n",
        "    return reranked_chunks, distances\n",
        "\n",
        "\n",
        "def judge_context_relevance(question: str, chunks: List[Result], distances: List[float]) -> RelevanceDecision:\n",
        "    \"\"\"Decide whether KB context is sufficient for a trustworthy answer.\"\"\"\n",
        "    if not chunks:\n",
        "        return RelevanceDecision(has_relevant_context=False, rationale=\"No retrieved chunks\")\n",
        "\n",
        "    if distances and min(distances) > MAX_KB_DISTANCE:\n",
        "        return RelevanceDecision(\n",
        "            has_relevant_context=False,\n",
        "            rationale=f\"Nearest chunk distance {min(distances):.3f} exceeds threshold\",\n",
        "        )\n",
        "\n",
        "    context_preview = \"\\n\\n\".join(chunk.page_content[:600] for chunk in chunks[:4])\n",
        "    structured_llm = llm.with_structured_output(RelevanceDecision)\n",
        "    return structured_llm.invoke(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=(\n",
        "                    \"You are a strict relevance judge. Determine if the provided knowledge-base context \"\n",
        "                    \"contains enough evidence to answer the user question accurately.\"\n",
        "                )\n",
        "            ),\n",
        "            HumanMessage(\n",
        "                content=(\n",
        "                    f\"Question:\\n{question}\\n\\n\"\n",
        "                    f\"Context:\\n{context_preview}\\n\\n\"\n",
        "                    \"Respond with has_relevant_context=true only when the context directly supports an answer.\"\n",
        "                )\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6ae8f34f",
      "metadata": {},
      "outputs": [],
      "source": [
        "KB_SYSTEM_PROMPT = \"\"\"\n",
        "You are a knowledgeable, friendly assistant representing Insurellm.\n",
        "Answer only from the provided Insurellm knowledge-base extracts.\n",
        "If information is missing from context, say you do not know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "WEB_SYSTEM_PROMPT = \"\"\"\n",
        "You are a helpful assistant.\n",
        "You are given raw web search findings.\n",
        "Summarize them accurately and clearly.\n",
        "If uncertain, say so.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_rag_messages(question: str, history: List[dict], chunks: List[Result]) -> List[dict]:\n",
        "    \"\"\"Create chat messages for KB-grounded final answer generation.\"\"\"\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"Extract from {chunk.metadata.get('source', 'unknown')}:\\n{chunk.page_content}\"\n",
        "        for chunk in chunks\n",
        "    )\n",
        "    system_prompt = KB_SYSTEM_PROMPT.format(context=context)\n",
        "    return [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "\n",
        "def build_langchain_messages(message_dicts: List[dict]) -> List[BaseMessage]:\n",
        "    \"\"\"Convert dictionary messages into LangChain message objects.\"\"\"\n",
        "    messages: List[BaseMessage] = []\n",
        "    for message in message_dicts:\n",
        "        role = message.get(\"role\", \"\").lower()\n",
        "        content = normalize_message_content(message.get(\"content\", \"\"))\n",
        "        if role == \"system\":\n",
        "            messages.append(SystemMessage(content=content))\n",
        "        elif role == \"user\":\n",
        "            messages.append(HumanMessage(content=content))\n",
        "        elif role == \"assistant\":\n",
        "            messages.append(AIMessage(content=content))\n",
        "    return messages\n",
        "\n",
        "\n",
        "def answer_from_web_stream(question: str):\n",
        "    \"\"\"Search the web and stream a transparent fallback answer.\"\"\"\n",
        "    web_results = web_search_tool.run(question)\n",
        "\n",
        "    intro = (\n",
        "        \"There is no relevant information about this in the Insurellm knowledge base. \"\n",
        "        \"Here is general information from the internet:\\n\\n\"\n",
        "    )\n",
        "    partial = intro\n",
        "    yield partial\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=WEB_SYSTEM_PROMPT.strip()),\n",
        "        HumanMessage(\n",
        "            content=(\n",
        "                f\"User question:\\n{question}\\n\\n\"\n",
        "                f\"Web search results:\\n{web_results}\\n\\n\"\n",
        "                \"Provide a concise, factual summary.\"\n",
        "            )\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    for chunk in llm.stream(messages):\n",
        "        if chunk.content:\n",
        "            partial += chunk.content\n",
        "            yield partial\n",
        "\n",
        "\n",
        "def answer_question_stream(question: str, history: Optional[List[dict]] = None):\n",
        "    \"\"\"Route question to KB-RAG or web fallback and stream output.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    rewritten_query = rewrite_query(question, history)\n",
        "    chunks, distances = fetch_context(rewritten_query)\n",
        "    relevance = judge_context_relevance(question, chunks, distances)\n",
        "\n",
        "    if not relevance.has_relevant_context:\n",
        "        for partial in answer_from_web_stream(question):\n",
        "            yield partial\n",
        "        return\n",
        "\n",
        "    message_dicts = make_rag_messages(question, history, chunks)\n",
        "    messages = build_langchain_messages(message_dicts)\n",
        "\n",
        "    partial_answer = \"\"\n",
        "    for chunk in llm.stream(messages):\n",
        "        if chunk.content:\n",
        "            partial_answer += chunk.content\n",
        "            yield partial_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4e60997d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradio_chat_handler(message: str, history: List[dict]):\n",
        "    \"\"\"Handle Gradio chat requests and stream agent responses.\"\"\"\n",
        "    rag_history = []\n",
        "    for item in history:\n",
        "        role = item.get(\"role\", \"\")\n",
        "        content = normalize_message_content(item.get(\"content\", \"\"))\n",
        "        if role in {\"user\", \"assistant\"} and content:\n",
        "            rag_history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    for partial_answer in answer_question_stream(message, rag_history):\n",
        "        yield partial_answer\n",
        "\n",
        "\n",
        "def check_vector_store_status() -> str:\n",
        "    \"\"\"Return a status string for the preprocessed vector store.\"\"\"\n",
        "    try:\n",
        "        collection = get_collection_or_raise()\n",
        "        return f\"Vector store ready: {collection.count()} chunks in '{COLLECTION_NAME}' from '{DB_NAME}'.\"\n",
        "    except Exception as exc:\n",
        "        return f\"Vector store error: {exc}\"\n",
        "\n",
        "\n",
        "def build_gradio_app() -> gr.Blocks:\n",
        "    \"\"\"Create the Gradio UI for the RAG + web-fallback agent.\"\"\"\n",
        "    with gr.Blocks(title=\"Insurellm RAG Agent\") as demo:\n",
        "        gr.Markdown(\"# Insurellm RAG Agent\")\n",
        "        gr.Markdown(\n",
        "            \"Uses preprocessed ChromaDB vectors for Insurellm answers and transparently falls back to web search when KB context is not relevant.\"\n",
        "        )\n",
        "\n",
        "        status_box = gr.Textbox(label=\"Vector Store Status\", interactive=False)\n",
        "        refresh_status_button = gr.Button(\"Refresh Vector Store Status\")\n",
        "\n",
        "        gr.ChatInterface(\n",
        "            fn=gradio_chat_handler,\n",
        "            title=\"Chat\",\n",
        "            description=\"Streaming responses enabled.\",\n",
        "        )\n",
        "\n",
        "        refresh_status_button.click(fn=check_vector_store_status, outputs=status_box)\n",
        "        demo.load(fn=check_vector_store_status, outputs=status_box)\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "demo_app = build_gradio_app()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc4fcd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_app.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
