{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "524535d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "from chromadb import PersistentClient\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
        "\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "MODEL = \"claude-haiku-4-5\"\n",
        "DB_NAME = \"preprocessed_db\"\n",
        "collection_name = \"docs\"\n",
        "embedding_model = \"all-MiniLM-L6-v2\"\n",
        "KNOWLEDGE_BASE_PATH = Path(\"knowledge-base\")\n",
        "AVERAGE_CHUNK_SIZE = 500\n",
        "RETRIEVAL_K = 10\n",
        "\n",
        "\n",
        "llm = ChatAnthropic(\n",
        "    model=MODEL,\n",
        "    temperature=0,\n",
        "    max_tokens=2000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "98a2c135",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Result(BaseModel):\n",
        "    \"\"\"Represents a retrievable chunk plus metadata.\"\"\"\n",
        "\n",
        "    page_content: str\n",
        "    metadata: dict\n",
        "\n",
        "\n",
        "class Chunk(BaseModel):\n",
        "    \"\"\"Represents one generated chunk from an input document.\"\"\"\n",
        "\n",
        "    headline: str = Field(\n",
        "        description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\"\n",
        "    )\n",
        "    summary: str = Field(\n",
        "        description=\"A few sentences summarizing the content of this chunk to answer common questions\"\n",
        "    )\n",
        "    original_text: str = Field(\n",
        "        description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\"\n",
        "    )\n",
        "\n",
        "    def as_result(self, document: dict) -> Result:\n",
        "        \"\"\"Convert a generated chunk into a retrievable result object.\"\"\"\n",
        "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
        "        page_content = f\"{self.headline}\\n\\n{self.summary}\\n\\n{self.original_text}\"\n",
        "        return Result(page_content=page_content, metadata=metadata)\n",
        "\n",
        "\n",
        "class Chunks(BaseModel):\n",
        "    \"\"\"Container model used for structured chunk extraction.\"\"\"\n",
        "\n",
        "    chunks: list[Chunk]\n",
        "\n",
        "\n",
        "class RankOrder(BaseModel):\n",
        "    \"\"\"Represents reranked chunk ids in descending relevance order.\"\"\"\n",
        "\n",
        "    order: list[int] = Field(\n",
        "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8fb45a18",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_documents() -> list[dict]:\n",
        "    \"\"\"Load markdown documents from the knowledge base directory.\"\"\"\n",
        "    documents = []\n",
        "\n",
        "    for folder in KNOWLEDGE_BASE_PATH.iterdir():\n",
        "        doc_type = folder.name\n",
        "        for file in folder.rglob(\"*.md\"):\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as file_handle:\n",
        "                documents.append(\n",
        "                    {\n",
        "                        \"type\": doc_type,\n",
        "                        \"source\": file.as_posix(),\n",
        "                        \"text\": file_handle.read(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    print(f\"Loaded {len(documents)} documents\")\n",
        "    return documents\n",
        "\n",
        "\n",
        "def make_prompt(document: dict) -> str:\n",
        "    \"\"\"Build the prompt used to split one source document into overlapping chunks.\"\"\"\n",
        "    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
        "    return f\"\"\"\n",
        "You take a document and you split the document into overlapping chunks for a KnowledgeBase.\n",
        "\n",
        "The document is from the shared drive of a company called Insurellm.\n",
        "The document is of type: {document[\"type\"]}\n",
        "The document has been retrieved from: {document[\"source\"]}\n",
        "\n",
        "A chatbot will use these chunks to answer questions about the company.\n",
        "You should divide up the document as you see fit, being sure that the entire document is returned in the chunks - don't leave anything out.\n",
        "This document should probably be split into {how_many} chunks, but you can have more or less as appropriate.\n",
        "There should be overlap between the chunks as appropriate; typically about 25% overlap or about 50 words, so you have the same text in multiple chunks for best retrieval results.\n",
        "\n",
        "For each chunk, you should provide a headline, a summary, and the original text of the chunk.\n",
        "Together your chunks should represent the entire document with overlap.\n",
        "\n",
        "Here is the document:\n",
        "\n",
        "{document[\"text\"]}\n",
        "\n",
        "Respond with the chunks.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_messages(document: dict) -> list[dict]:\n",
        "    \"\"\"Create message payload for the chunking LLM call.\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": make_prompt(document)}]\n",
        "\n",
        "\n",
        "def process_document(document: dict) -> list[Result]:\n",
        "    \"\"\"Generate structured chunks for a single document and return retrievable results.\"\"\"\n",
        "    structured_llm = llm.with_structured_output(Chunks)\n",
        "    messages = make_messages(document)\n",
        "    doc_as_chunks_obj = structured_llm.invoke(messages)\n",
        "    return [chunk.as_result(document) for chunk in doc_as_chunks_obj.chunks]\n",
        "\n",
        "\n",
        "def create_chunks(documents: list[dict]) -> list[Result]:\n",
        "    \"\"\"Process all loaded documents into chunk results.\"\"\"\n",
        "    chunks = []\n",
        "    for document in tqdm(documents):\n",
        "        chunks.extend(process_document(document))\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def create_embeddings(chunks: list[Result]) -> None:\n",
        "    \"\"\"Embed chunk texts and store them in a persistent ChromaDB collection.\"\"\"\n",
        "    chroma = PersistentClient(path=DB_NAME)\n",
        "    if collection_name in [collection.name for collection in chroma.list_collections()]:\n",
        "        chroma.delete_collection(collection_name)\n",
        "\n",
        "    texts = [chunk.page_content for chunk in chunks]\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "    vectors = embeddings.embed_documents(texts)\n",
        "\n",
        "    collection = chroma.get_or_create_collection(collection_name)\n",
        "    ids = [str(index) for index in range(len(chunks))]\n",
        "    metadatas = [chunk.metadata for chunk in chunks]\n",
        "\n",
        "    collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metadatas)\n",
        "    print(f\"Vectorstore created with {collection.count()} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "718929ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_context_unranked(question: str) -> list[Result]:\n",
        "    \"\"\"Retrieve top-k nearest chunks from ChromaDB before reranking.\"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "    query_vector = embeddings.embed_query(question)\n",
        "\n",
        "    chroma = PersistentClient(path=DB_NAME)\n",
        "    collection = chroma.get_or_create_collection(collection_name)\n",
        "    results = collection.query(query_embeddings=[query_vector], n_results=RETRIEVAL_K)\n",
        "\n",
        "    chunks = []\n",
        "    for document, metadata in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
        "        chunks.append(Result(page_content=document, metadata=metadata))\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def rerank(question: str, chunks: List[Result]) -> List[Result]:\n",
        "    \"\"\"Rerank retrieved chunks by relevance using Claude structured output.\"\"\"\n",
        "    system_prompt = \"\"\"\n",
        "You are a document re-ranker.\n",
        "You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
        "The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance,\n",
        "but you may be able to improve on that.\n",
        "\n",
        "You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
        "\n",
        "Reply ONLY with valid JSON in the following format:\n",
        "{\"order\": [<chunk_id_1>, <chunk_id_2>, ...]}\n",
        "\n",
        "Include all the chunk ids you are provided with, reranked.\n",
        "\"\"\"\n",
        "\n",
        "    user_prompt_lines = [\n",
        "        f\"The user has asked the following question:\\n\\n{question}\\n\",\n",
        "        \"Order all the chunks of text by relevance to the question, from most relevant to least relevant.\",\n",
        "        \"Here are the chunks:\\n\",\n",
        "    ]\n",
        "\n",
        "    for index, chunk in enumerate(chunks, start=1):\n",
        "        user_prompt_lines.append(f\"# CHUNK ID: {index}\\n\\n{chunk.page_content}\\n\")\n",
        "\n",
        "    user_prompt = \"\\n\".join(user_prompt_lines)\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt.strip()),\n",
        "        HumanMessage(content=user_prompt.strip()),\n",
        "    ]\n",
        "\n",
        "    structured_llm = llm.with_structured_output(RankOrder)\n",
        "    rank_order = structured_llm.invoke(messages)\n",
        "    order = rank_order.order\n",
        "    valid_order = [index for index in order if 1 <= index <= len(chunks)]\n",
        "    if not valid_order:\n",
        "        return chunks\n",
        "    return [chunks[index - 1] for index in valid_order]\n",
        "\n",
        "\n",
        "def fetch_context(question: str) -> list[Result]:\n",
        "    \"\"\"Retrieve and rerank chunks for a user question.\"\"\"\n",
        "    chunks = fetch_context_unranked(question)\n",
        "    return rerank(question, chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e16dfb5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_message_content(content: object) -> str:\n",
        "    \"\"\"Normalize chat content values from Gradio/LangChain into plain text.\"\"\"\n",
        "    if isinstance(content, str):\n",
        "        return content.strip()\n",
        "\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for item in content:\n",
        "            if isinstance(item, str):\n",
        "                text = item.strip()\n",
        "                if text:\n",
        "                    parts.append(text)\n",
        "            elif isinstance(item, dict):\n",
        "                text = str(item.get(\"text\", \"\")).strip()\n",
        "                if text:\n",
        "                    parts.append(text)\n",
        "        return \"\\n\".join(parts).strip()\n",
        "\n",
        "    if isinstance(content, dict):\n",
        "        return str(content.get(\"text\", \"\")).strip()\n",
        "\n",
        "    return str(content).strip()\n",
        "\n",
        "\n",
        "def rewrite_query(question: str, history: Optional[List[dict]] = None) -> str:\n",
        "    \"\"\"Rewrite a user question into a focused knowledge-base retrieval query.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    history_str = \"\"\n",
        "    if history:\n",
        "        lines = []\n",
        "        for message in history:\n",
        "            role = \"User\" if message.get(\"role\", \"\").lower() == \"user\" else \"Assistant\"\n",
        "            content = normalize_message_content(message.get(\"content\", \"\"))\n",
        "            if content:\n",
        "                lines.append(f\"{role}: {content}\")\n",
        "        history_str = \"\\n\".join(lines)\n",
        "    else:\n",
        "        history_str = \"(no history)\"\n",
        "\n",
        "    system_content = f\"\"\"\n",
        "You are in a conversation with a user, answering questions about the company Insurellm.\n",
        "You are about to look up information in a Knowledge Base to answer the user's question.\n",
        "\n",
        "This is the history of your conversation so far with the user:\n",
        "{history_str}\n",
        "\n",
        "And this is the user's current question:\n",
        "{question}\n",
        "\n",
        "Respond only with a single, refined question that you will use to search the Knowledge Base.\n",
        "It should be a VERY short specific question most likely to surface content. Focus on the question details.\n",
        "Don't mention the company name unless it's a general question about the company.\n",
        "IMPORTANT: Respond ONLY with the knowledgebase query, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_content),\n",
        "        HumanMessage(content=\"Please rewrite now.\"),\n",
        "    ]\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "    query = response.content.strip()\n",
        "\n",
        "    for bad_start in [\"Query:\", \"Search:\", '\"', \"Refined query:\", \"Here is the query:\"]:\n",
        "        if query.startswith(bad_start):\n",
        "            query = query[len(bad_start):].strip()\n",
        "\n",
        "    return query.strip('\"').strip()\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
        "You are chatting with a user about Insurellm.\n",
        "Your answer will be evaluated for accuracy, relevance and completeness, so make sure it only answers the question and fully answers it.\n",
        "If you don't know the answer, say so.\n",
        "For context, here are specific extracts from the Knowledge Base that might be directly relevant to the user's question:\n",
        "{context}\n",
        "\n",
        "With this context, please answer the user's question. Be accurate, relevant and complete.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_rag_messages(question: str, history: List[dict], chunks: List[Result]) -> List[dict]:\n",
        "    \"\"\"Create chat messages for final answer generation using retrieved context.\"\"\"\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"Extract from {chunk.metadata['source']}:\\n{chunk.page_content}\" for chunk in chunks\n",
        "    )\n",
        "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
        "    return [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "\n",
        "def build_langchain_messages(message_dicts: List[dict]) -> List[BaseMessage]:\n",
        "    \"\"\"Convert dictionary messages into LangChain message objects.\"\"\"\n",
        "    messages: List[BaseMessage] = []\n",
        "    for message in message_dicts:\n",
        "        role = message.get(\"role\", \"\").lower()\n",
        "        content = message.get(\"content\", \"\")\n",
        "        if role == \"system\":\n",
        "            messages.append(SystemMessage(content=content))\n",
        "        elif role == \"user\":\n",
        "            messages.append(HumanMessage(content=content))\n",
        "        elif role == \"assistant\":\n",
        "            messages.append(AIMessage(content=content))\n",
        "    return messages\n",
        "\n",
        "\n",
        "def answer_question(question: str, history: Optional[List[dict]] = None) -> Tuple[str, List[Result]]:\n",
        "    \"\"\"Run full RAG flow: rewrite, retrieve, rerank, and answer.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    query = rewrite_query(question, history)\n",
        "    chunks = fetch_context(query)\n",
        "    message_dicts = make_rag_messages(question, history, chunks)\n",
        "    messages = build_langchain_messages(message_dicts)\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "    answer = response.content.strip()\n",
        "    return answer, chunks\n",
        "\n",
        "\n",
        "def answer_question_stream(question: str, history: Optional[List[dict]] = None):\n",
        "    \"\"\"Run full RAG flow and stream the final answer token-by-token.\"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    query = rewrite_query(question, history)\n",
        "    chunks = fetch_context(query)\n",
        "    message_dicts = make_rag_messages(question, history, chunks)\n",
        "    messages = build_langchain_messages(message_dicts)\n",
        "\n",
        "    partial_answer = \"\"\n",
        "    for chunk in llm.stream(messages):\n",
        "        if chunk.content:\n",
        "            partial_answer += chunk.content\n",
        "            yield partial_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dff9d4f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_knowledge_base_ready() -> str:\n",
        "    \"\"\"Ensure the ChromaDB collection exists and has indexed documents.\"\"\"\n",
        "    chroma = PersistentClient(path=DB_NAME)\n",
        "    collection_names = [collection.name for collection in chroma.list_collections()]\n",
        "\n",
        "    if collection_name in collection_names:\n",
        "        collection = chroma.get_or_create_collection(collection_name)\n",
        "        if collection.count() > 0:\n",
        "            return f\"Knowledge base ready with {collection.count()} chunks.\"\n",
        "\n",
        "    documents = fetch_documents()\n",
        "    chunks = create_chunks(documents)\n",
        "    create_embeddings(chunks)\n",
        "\n",
        "    collection = chroma.get_or_create_collection(collection_name)\n",
        "    return f\"Knowledge base built with {collection.count()} chunks.\"\n",
        "\n",
        "\n",
        "def gradio_chat_handler(message: str, history: List[dict]):\n",
        "    \"\"\"Handle Gradio chat requests and stream RAG answers.\"\"\"\n",
        "    rag_history = []\n",
        "    for history_item in history:\n",
        "        role = history_item.get(\"role\", \"\")\n",
        "        content = normalize_message_content(history_item.get(\"content\", \"\"))\n",
        "        if role in {\"user\", \"assistant\"} and content:\n",
        "            rag_history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    for partial_answer in answer_question_stream(message, rag_history):\n",
        "        yield partial_answer\n",
        "\n",
        "\n",
        "def build_gradio_app() -> gr.Blocks:\n",
        "    \"\"\"Create the Gradio UI for the RAG chat assistant.\"\"\"\n",
        "    with gr.Blocks(title=\"Insurellm RAG Assistant\") as demo:\n",
        "        gr.Markdown(\"# Insurellm RAG Assistant\")\n",
        "        gr.Markdown(\"Ask questions about Insurellm and get streamed RAG responses.\")\n",
        "\n",
        "        status_box = gr.Textbox(label=\"Knowledge Base Status\", interactive=False)\n",
        "        init_button = gr.Button(\"Initialize / Refresh Knowledge Base\")\n",
        "\n",
        "        chat_interface = gr.ChatInterface(\n",
        "            fn=gradio_chat_handler,\n",
        "            title=\"RAG Chat\",\n",
        "            description=\"Responses are generated with retrieval, reranking, and streaming output.\",\n",
        "        )\n",
        "\n",
        "        init_button.click(fn=ensure_knowledge_base_ready, outputs=status_box)\n",
        "        demo.load(fn=ensure_knowledge_base_ready, outputs=status_box)\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "demo_app = build_gradio_app()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9621769d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_app.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
