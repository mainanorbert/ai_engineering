{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "42f98399",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "860e38e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key not set\n",
            "Anthropic API Key exists and begins sk-ant-\n",
            "Google API Key not set (and this is optional)\n",
            "DeepSeek API Key not set (and this is optional)\n",
            "Groq API Key not set (and this is optional)\n",
            "Grok API Key not set (and this is optional)\n",
            "OpenRouter API Key exists and begins sk-\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')\n",
        "grok_api_key = os.getenv('GROK_API_KEY')\n",
        "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "    \n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
        "else:\n",
        "    print(\"Anthropic API Key not set (and this is optional)\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set (and this is optional)\")\n",
        "\n",
        "if deepseek_api_key:\n",
        "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
        "else:\n",
        "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
        "\n",
        "if groq_api_key:\n",
        "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
        "else:\n",
        "    print(\"Groq API Key not set (and this is optional)\")\n",
        "\n",
        "if grok_api_key:\n",
        "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
        "else:\n",
        "    print(\"Grok API Key not set (and this is optional)\")\n",
        "\n",
        "if openrouter_api_key:\n",
        "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:3]}\")\n",
        "else:\n",
        "    print(\"OpenRouter API Key not set (and this is optional)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5bdeb6c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/3\n"
          ]
        }
      ],
      "source": [
        "# Using open router with any model\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openrouter_api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")\n",
        "\n",
        "easy_puzzle = [\n",
        "    {\"role\": \"user\", \"content\": \n",
        "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model    = \"anthropic/claude-sonnet-4.6\",   # ‚Üê change only here\n",
        "    messages = easy_puzzle,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daf8b073",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Testing various models with same question\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    api_key=openrouter_api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")\n",
        "\n",
        "hard = \"\"\"\n",
        "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
        "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
        "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
        "What distance did it gnaw through?\n",
        "\"\"\"\n",
        "hard_puzzle = [\n",
        "    {\"role\": \"user\", \"content\": hard}\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "## Using openai/gpt-4o-mini\n",
        "\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    messages=hard_puzzle, reasoning_effort=\"minimal\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))\n",
        "\n",
        "print(\"Using google/gemini-2.0-flash-lite.........\")\n",
        "\n",
        "\n",
        "## Using google/gemini-2.0-flash-lite-001   \n",
        "\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"google/gemini-2.0-flash-lite-001\",\n",
        "    messages=hard_puzzle, reasoning_effort=\"minimal\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))\n",
        "\n",
        "\n",
        "print(\"Using anthropic/claude-3-5-sonnet-20240620.........\")\n",
        "\n",
        "\n",
        "## Using anthropic/claude-haiku-4-5\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"anthropic/claude-haiku-4-5\",\n",
        "    messages=hard_puzzle, reasoning_effort=\"minimal\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "51725dd1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'Ollama is running'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "requests.get(\"http://localhost:11434/\").content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7b1fd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "easy_puzzle = [\n",
        "    {\"role\": \"user\", \"content\": \n",
        "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
        "]\n",
        "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
        "display(Markdown(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c3f8a60d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Blue is the color that feels like cool water on your skin, sounds like calm ocean waves, and carries the peaceful quiet of early morning air.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from anthropic import Anthropic\n",
        "\n",
        "client = Anthropic()\n",
        "\n",
        "response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-5-20250929\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
        "    max_tokens=100\n",
        ")\n",
        "print(response.content[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6033bbc8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Why did the LLM engineer break up with their dataset?\n",
              "\n",
              "Because it had too many \"issues\" and couldn't stop \"overfitting\" on past relationships!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Point LangChain(OpenAI) to OpenRouter\n",
        "if not openrouter_api_key:\n",
        "    raise ValueError(\"OPENROUTER_API_KEY is not set\")\n",
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"openai/gpt-4o-mini\")\n",
        "\n",
        "tell_a_joke=\"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"\n",
        "response = llm.invoke(tell_a_joke)\n",
        "\n",
        "display(Markdown(response.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30686583",
      "metadata": {},
      "source": [
        "## Lightweight LiteLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b8492206",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Why did the language model go to therapy?\n",
              "\n",
              "Because it had too many unresolved prompts!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from litellm import completion\n",
        "\n",
        "if not openrouter_api_key:\n",
        "    raise ValueError(\"OPENROUTER_API_KEY is not set\")\n",
        "\n",
        "# LiteLLM expects OpenAI-style chat messages (list of dicts)\n",
        "messages = tell_a_joke\n",
        "if isinstance(messages, str):\n",
        "    messages = [{\"role\": \"user\", \"content\": messages}]\n",
        "\n",
        "response = completion(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    api_key=openrouter_api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")\n",
        "\n",
        "reply = response.choices[0].message.content\n",
        "display(Markdown(reply))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebdd690",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tokens: 24\n",
            "Output tokens: 17\n",
            "Total tokens: 41\n",
            "Total cost: 0.0014 cents\n"
          ]
        }
      ],
      "source": [
        "# ---- Token and Cost Reporting ----\n",
        "# Extract usage information (token counts) from the response object.\n",
        "# 'usage' can be either a dict (OpenAI-style) or an object with attributes (LiteLLM/other wrappers).\n",
        "usage = getattr(response, \"usage\", None) or {}\n",
        "\n",
        "\n",
        "# Safely retrieve prompt, completion, and total tokens regardless of usage type.\n",
        "prompt_tokens = usage.get(\"prompt_tokens\") if isinstance(usage, dict) else getattr(usage, \"prompt_tokens\", None)\n",
        "completion_tokens = usage.get(\"completion_tokens\") if isinstance(usage, dict) else getattr(usage, \"completion_tokens\", None)\n",
        "total_tokens = usage.get(\"total_tokens\") if isinstance(usage, dict) else getattr(usage, \"total_tokens\", None)\n",
        "\n",
        "# Print token usage statistics for the request\n",
        "print(f\"Input tokens: {prompt_tokens}\")        # Number of tokens in the prompt (input)\n",
        "print(f\"Output tokens: {completion_tokens}\")   # Number of tokens generated by the model (output)\n",
        "print(f\"Total tokens: {total_tokens}\")         # Combined input and output tokens\n",
        "\n",
        "# Extract cost information if present.\n",
        "# Some LiteLLM responses include a hidden dict \"_hidden_params\" holding \"response_cost\" (float, in dollars).\n",
        "hidden = getattr(response, \"_hidden_params\", None) or {}\n",
        "response_cost = hidden.get(\"response_cost\") if isinstance(hidden, dict) else None\n",
        "\n",
        "# Print total cost in cents if available, otherwise print 'n/a'.\n",
        "if response_cost is None:\n",
        "    print(\"Total cost: n/a\")\n",
        "else:\n",
        "    print(f\"Total cost: {response_cost * 100:.4f} cents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec14f95",
      "metadata": {},
      "source": [
        "## Prompt caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8bc4440",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "Speak, man\n",
            "by William Shakespeare\n",
            "\n",
            "[Public domain]\n",
            "\n",
            "ACT I\n",
            "SCENE I. Elsinore. A platform before the c\n"
          ]
        }
      ],
      "source": [
        "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    hamlet = f.read()\n",
        "\n",
        "#Location index where except is found\n",
        "loc = hamlet.find(\"Speak, man\")\n",
        "print(loc)\n",
        "print(hamlet[loc:loc+100])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d83e08ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "30bfa81b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "In Shakespeare's \"Hamlet,\" when Laertes asks \"Where is my father?\", he is referring to Polonius, his father. The response comes from Gertrude, who is shocked and distressed. In Act IV, Scene 5, she tells Laertes that Polonius is dead. This moment is significant as it sets off a chain of events that lead to further conflict in the play. If you would like more details about the context or themes surrounding this moment, feel free to ask!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = completion(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    messages=question,\n",
        "    api_key=openrouter_api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "f89f620b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tokens: 25\n",
            "Output tokens: 102\n",
            "Total tokens: 127\n",
            "Total cost: 0.0065 cents\n"
          ]
        }
      ],
      "source": [
        "# ---- Token and Cost Reporting ----\n",
        "# Extract usage information (token counts) from the response object.\n",
        "# 'usage' can be either a dict (OpenAI-style) or an object with attributes (LiteLLM/other wrappers).\n",
        "usage = getattr(response, \"usage\", None) or {}\n",
        "\n",
        "\n",
        "# Safely retrieve prompt, completion, and total tokens regardless of usage type.\n",
        "prompt_tokens = usage.get(\"prompt_tokens\") if isinstance(usage, dict) else getattr(usage, \"prompt_tokens\", None)\n",
        "completion_tokens = usage.get(\"completion_tokens\") if isinstance(usage, dict) else getattr(usage, \"completion_tokens\", None)\n",
        "total_tokens = usage.get(\"total_tokens\") if isinstance(usage, dict) else getattr(usage, \"total_tokens\", None)\n",
        "\n",
        "# Print token usage statistics for the request\n",
        "print(f\"Input tokens: {prompt_tokens}\")        # Number of tokens in the prompt (input)\n",
        "print(f\"Output tokens: {completion_tokens}\")   # Number of tokens generated by the model (output)\n",
        "print(f\"Total tokens: {total_tokens}\")         # Combined input and output tokens\n",
        "\n",
        "# Extract cost information if present.\n",
        "# Some LiteLLM responses include a hidden dict \"_hidden_params\" holding \"response_cost\" (float, in dollars).\n",
        "hidden = getattr(response, \"_hidden_params\", None) or {}\n",
        "response_cost = hidden.get(\"response_cost\") if isinstance(hidden, dict) else None\n",
        "\n",
        "# Print total cost in cents if available, otherwise print 'n/a'.\n",
        "if response_cost is None:\n",
        "    print(\"Total cost: n/a\")\n",
        "else:\n",
        "    print(f\"Total cost: {response_cost * 100:.4f} cents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "b3775db4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'user', 'content': \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\\n\\nFor context, here is the entire text of Hamlet:\\n\\nHAMLET  Speak, man\\nby William Shakespeare\\n\\n[Public domain]\\n\\nACT I\\nSCENE I. Elsinore. A platform before the castle.\\n\\nFRANCISCO at his post. Enter to him BERNARDO.\\n\\nBERNARDO:\\nWho's there?\\n\\nFRANCISCO:\\nNay, answer me. Stand, and unfold yourself.\\n\\nBERNARDO:\\nLong live the King!\\n\\nFRANCISCO:\\nBernardo?\\n\\nBERNARDO:\\nHe.\\n\\nFRANCISCO:\\nYou come most carefully upon your hour.\\n\\nBERNARDO:\\n'Tis now struck twelve. Get thee to bed, Francisco.\\n\\nFor context, here is the entire text of Hamlet:\\n\\nHAMLET  Speak, man\\nby William Shakespeare\\n\\n[Public domain]\\n\\nACT I\\nSCENE I. Elsinore. A platform before the castle.\\n\\nFRANCISCO at his post. Enter to him BERNARDO.\\n\\nBERNARDO:\\nWho's there?\\n\\nFRANCISCO:\\nNay, answer me. Stand, and unfold yourself.\\n\\nBERNARDO:\\nLong live the King!\\n\\nFRANCISCO:\\nBernardo?\\n\\nBERNARDO:\\nHe.\\n\\nFRANCISCO:\\nYou come most carefully upon your hour.\\n\\nBERNARDO:\\n'Tis now struck twelve. Get thee to bed, Francisco.\"}]\n"
          ]
        }
      ],
      "source": [
        "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet\n",
        "\n",
        "print(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "43ed1873",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "In \"Hamlet,\" when Laertes asks, \"Where is my father?\" he is referring to Polonius. He receives the reply, \"At home, my lord,\" from Ophelia. This exchange occurs in Act 2, Scene 1, when Laertes has just returned to Denmark and is inquiring about his father."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = completion(model=\"openai/gpt-4o-mini\",\n",
        "    api_key=openrouter_api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\", messages=question)\n",
        "display(Markdown(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "f9efa804",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tokens: 161\n",
            "Output tokens: 69\n",
            "Total tokens: 230\n",
            "Total cost: 0.0066 cents\n"
          ]
        }
      ],
      "source": [
        "# ---- Token and Cost Reporting ----\n",
        "# Extract usage information (token counts) from the response object.\n",
        "# 'usage' can be either a dict (OpenAI-style) or an object with attributes (LiteLLM/other wrappers).\n",
        "usage = getattr(response, \"usage\", None) or {}\n",
        "\n",
        "\n",
        "# Safely retrieve prompt, completion, and total tokens regardless of usage type.\n",
        "prompt_tokens = usage.get(\"prompt_tokens\") if isinstance(usage, dict) else getattr(usage, \"prompt_tokens\", None)\n",
        "completion_tokens = usage.get(\"completion_tokens\") if isinstance(usage, dict) else getattr(usage, \"completion_tokens\", None)\n",
        "total_tokens = usage.get(\"total_tokens\") if isinstance(usage, dict) else getattr(usage, \"total_tokens\", None)\n",
        "\n",
        "# Print token usage statistics for the request\n",
        "print(f\"Input tokens: {prompt_tokens}\")        # Number of tokens in the prompt (input)\n",
        "print(f\"Output tokens: {completion_tokens}\")   # Number of tokens generated by the model (output)\n",
        "print(f\"Total tokens: {total_tokens}\")         # Combined input and output tokens\n",
        "\n",
        "# Extract cost information if present.\n",
        "# Some LiteLLM responses include a hidden dict \"_hidden_params\" holding \"response_cost\" (float, in dollars).\n",
        "hidden = getattr(response, \"_hidden_params\", None) or {}\n",
        "response_cost = hidden.get(\"response_cost\") if isinstance(hidden, dict) else None\n",
        "\n",
        "# Print total cost in cents if available, otherwise print 'n/a'.\n",
        "if response_cost is None:\n",
        "    print(\"Total cost: n/a\")\n",
        "else:\n",
        "    print(f\"Total cost: {response_cost * 100:.4f} cents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "2c7b6a28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's make a conversation between GPT-4.1-mini and Claude-haiku-4.5\n",
        "# We're using cheap versions of models so the costs will be minimal\n",
        "\n",
        "gpt_model = \"gpt-4.1-mini\"\n",
        "claude_model = \"claude-haiku-4-5\"\n",
        "\n",
        "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
        "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
        "\n",
        "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
        "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
        "you try to calm them down and keep chatting.\"\n",
        "\n",
        "gpt_messages = [\"Hi there\"]\n",
        "claude_messages = [\"Hi\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9904778f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def call_gpt():\n",
        "    # Initialize the OpenAI client (uses OPENAI_API_KEY from your environment)\n",
        "    client = OpenAI()\n",
        "\n",
        "    # Prepare the conversation as a list of messages\n",
        "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
        "    # Loop over pairs of messages from the GPT and Claude chatbots.\n",
        "    # For each turn, 'gpt' is the GPT-4.1-mini response and 'claude' is the Claude-haiku-4.5 response.\n",
        "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
        "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
        "        messages.append({\"role\": \"user\", \"content\": claude})\n",
        "        print(messages)\n",
        "\n",
        "    # Create the chat completion using the specified model and messages\n",
        "    response = client.chat.completions.create(model=gpt_model, messages=messages)\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "fb483745",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}]\n"
          ]
        }
      ],
      "source": [
        "call_gpt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "8fd162d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def call_claude():\n",
        "    # Use the OpenRouter API key from the environment or .env file\n",
        "    openrouter_api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
        "    client = OpenAI(\n",
        "        api_key=openrouter_api_key,\n",
        "        base_url=\"https://openrouter.ai/api/v1\"\n",
        "    )\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
        "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
        "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
        "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
        "\n",
        "    # Use OpenRouter's model identifier for Claude Haiku 4.5\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"anthropic/claude-3-haiku:beta\",\n",
        "        messages=messages\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "c892670e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello! How are you doing today?'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "call_claude()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "65f538b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}]\n"
          ]
        }
      ],
      "source": [
        "call_gpt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "f9c95d28",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "### GPT:\n",
              "Hi there\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### Claude:\n",
              "Hi\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}]\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### GPT:\n",
              "None\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### Claude:\n",
              " there! How are you doing today? I'm here to chat and help out however I can.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}]\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### GPT:\n",
              "None\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### Claude:\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}]\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### GPT:\n",
              "None\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### Claude:\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}]\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### GPT:\n",
              "None\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### Claude:\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}]\n",
            "[{'role': 'system', 'content': 'You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.'}, {'role': 'assistant', 'content': 'Hi there'}, {'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': \" there! How are you doing today? I'm here to chat and help out however I can.\"}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}, {'role': 'assistant', 'content': None}, {'role': 'user', 'content': ''}]\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### GPT:\n",
              "None\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### Claude:\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "gpt_messages = [\"Hi there\"]\n",
        "claude_messages = [\"Hi\"]\n",
        "\n",
        "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
        "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
        "\n",
        "for i in range(5):\n",
        "    gpt_next = call_gpt()\n",
        "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
        "    gpt_messages.append(gpt_next)\n",
        "    \n",
        "    claude_next = call_claude()\n",
        "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
        "    claude_messages.append(claude_next)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
